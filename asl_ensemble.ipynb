{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import splitfolders\n",
    "\n",
    "from tensorflow.keras import models, layers, Input\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('datasets', exist_ok=True)\n",
    "\n",
    "# List files in a directory as a sanity check\n",
    "\"\"\"for dirname, _, filenames in os.walk('Data/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\"\"\"\n",
    "\n",
    "# List files in a specific directory\n",
    "# This will list the labels\n",
    "print(os.listdir('Data/kaggle/input/asl-alphabet/asl_alphabet_train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train_src = \"Data/kaggle/input/asl-alphabet/asl_alphabet_train/\"\n",
    "\n",
    "splitfolders.ratio(train_src, output=\"datasets/asl_alphabet\",\n",
    "    seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False) # default values\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'datasets/asl_alphabet/train'\n",
    "val_dir = 'datasets/asl_alphabet/val'\n",
    "test_dir  = 'datasets/asl_alphabet/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "target_size = (32,32) # dataset pic = 200x200\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
    "val_datagen   = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
    "test_datagen  = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"rgb\",\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"rgb\",\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"rgb\",\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(train_generator.class_indices.keys())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = train_dir\n",
    "def sample_images(labels):\n",
    "    # Create Subplots\n",
    "    y_size = 12\n",
    "    if(len(labels)<10):\n",
    "        y_size = y_size * len(labels) / 10\n",
    "    fig, axs = plt.subplots(len(labels), 9, figsize=(y_size, 13))\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        axs[i, 0].text(0.5, 0.5, label, ha='center', va='center', fontsize=8)\n",
    "        axs[i, 0].axis('off')\n",
    "\n",
    "        label_path = os.path.join(TRAIN_PATH, label)\n",
    "        list_files = os.listdir(label_path)\n",
    "\n",
    "        for j in range(8):\n",
    "            img_label = cv2.imread(os.path.join(label_path, list_files[j]))\n",
    "            img_label = cv2.cvtColor(img_label, cv2.COLOR_BGR2RGB)\n",
    "            axs[i, j+1].imshow(img_label)\n",
    "            axs[i, j+1].axis(\"off\")\n",
    "\n",
    "    # Title\n",
    "    plt.suptitle(\"Sample Images in ASL Alphabet Dataset\", x=0.55, y=0.92)\n",
    "\n",
    "    # Show\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images(labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(labels)\n",
    "input_shape = (32,32,3)\n",
    "\n",
    "input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "# Build Model\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(input_layer)\n",
    "\n",
    "# 1st convolution layer\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "# 2nd convolution layer\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "# 3rd convolution layer\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "# fully-connected layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile Model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint path\n",
    "checkpoint_path = \"best_model.keras\"\n",
    "\n",
    "# Create ModelCheckpoint callback\n",
    "checkpoint = ModelCheckpoint(checkpoint_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, validation_data=val_generator, epochs=10, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_generator) \n",
    "print('Test loss: ', scores[0])\n",
    "print('Test accuracy: ', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"asl_alphabet_cnn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNet Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "batch_size = 32\n",
    "num_classes = 29\n",
    "\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "output = Dense(num_classes, activation='softmax')(x)  # Output layer with softmax activation\n",
    "mobile_model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=5, batch_size=32, validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint path\n",
    "checkpoint_path = \"best_model.keras\"\n",
    "\n",
    "# Create ModelCheckpoint callback\n",
    "checkpoint = ModelCheckpoint(checkpoint_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "model.save(\"asl_alphabet_mobilenet.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(generator):\n",
    "    flattened_data_batches = []\n",
    "    label_batches = []\n",
    "\n",
    "    for i in range(len(generator)):\n",
    "        batch_x, batch_y = generator[i]\n",
    "        batch_x_flat = batch_x.reshape(batch_x.shape[0], -1)\n",
    "        \n",
    "        flattened_data_batches.append(batch_x_flat)\n",
    "        label_batches.append(batch_y)\n",
    "\n",
    "        print(f\"Flattening progress: {i+1} of {len(generator)} batches\", end=\"\\r\", flush=True)\n",
    "\n",
    "    return flattened_data_batches, label_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_flat_batches, train_y_batches = flatten_data(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x_flat_batches, val_y_batches = flatten_data(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep test data\n",
    "test_x_flat_batches, test_y = flatten_data(test_generator)\n",
    "test_x_flat = np.concatenate(test_x_flat_batches)\n",
    "test_y = np.concatenate(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_david = 'datasets/asl_alphabet/test_david'\n",
    "\n",
    "test_david_datagenerator = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 15,\n",
    "    fill_mode = 'nearest',\n",
    "    horizontal_flip = True\n",
    "    #width_shift_range = 0.2,\n",
    "    #height_shift_range = 0.2\n",
    "    )\n",
    "\n",
    "test_david_generator = test_david_datagenerator.flow_from_directory(\n",
    "    test_david,\n",
    "    target_size = (32, 32),\n",
    "    class_mode = 'categorical',\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# david test flatten\n",
    "test_d_flat_batches, test_d_y = flatten_data(test_david_generator)\n",
    "test_d_flat = np.concatenate(test_d_flat_batches)\n",
    "test_d_y = np.concatenate(test_d_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate flattened data and label batches\n",
    "train_x_flat = np.concatenate(train_x_flat_batches)\n",
    "train_y = np.concatenate(train_y_batches)\n",
    "val_x_flat = np.concatenate(val_x_flat_batches)\n",
    "val_y = np.concatenate(val_y_batches)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(train_x_flat, train_y)\n",
    "train_accuracy = knn.score(train_x_flat, train_y)\n",
    "print(\"Training Accuracy: \", train_accuracy)\n",
    "\n",
    "# Evaluate the model\n",
    "val_accuracy = knn.score(val_x_flat, val_y)\n",
    "print(\"Validation Accuracy: \", val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and freeze VGG model \n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(32, 32, 3), pooling='max')\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom top layers\n",
    "inputs = Input(shape=(32,32,3))\n",
    "feature_maps = base_model(inputs, training=False)\n",
    "\n",
    "dense_layer_1 = Dense(512, activation='leaky_relu', kernel_regularizer=l2(0.0001), kernel_initializer='he_normal')(feature_maps)\n",
    "dropout_1 = Dropout(0.2)(dense_layer_1)\n",
    "dense_layer_2 = Dense(256, activation='leaky_relu', kernel_regularizer=l2(0.0001), kernel_initializer='he_normal')(dropout_1)\n",
    "dropout_2 = Dropout(0.2)(dense_layer_2)\n",
    "dense_layer_3 = Dense(128, activation='leaky_relu', kernel_regularizer=l2(0.0001), kernel_initializer='he_normal')(dropout_2)\n",
    "dropout_3 = Dropout(0.2)(dense_layer_3)\n",
    "predictions = Dense(29, activation='softmax',kernel_regularizer=l2(0.0001))(dropout_3)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.compile(optimizer=Nadam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    batch_size = 32,\n",
    "    epochs= 10,\n",
    "    validation_data=(val_generator),\n",
    "    shuffle=True,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions_knn = knn.predict(test_x_flat)\n",
    "predictions_knn = knn.predict(test_d_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved models\n",
    "model_cnn = load_model(\"asl_alphabet_cnn.h5\")\n",
    "model_mobilenet = load_model(\"asl_alphabet_mobilenet.h5\")\n",
    "\n",
    "true_labels = test_david_generator.classes\n",
    "\n",
    "num_train_samples = len(train_generator)\n",
    "num_val_samples = len(val_generator)\n",
    "\n",
    "predictions_cnn = model_cnn.predict(test_david_generator)\n",
    "predictions_mobilenet = model_mobilenet.predict(test_david_generator)\n",
    "predictions_vgg = model.predict(test_david_generator)\n",
    "\n",
    "combined_predictions = (predictions_cnn + predictions_mobilenet + predictions_knn + predictions_vgg) / 4\n",
    "\n",
    "ensemble_labels = np.argmax(combined_predictions, axis=1)\n",
    "\n",
    "ensemble_accuracy_combined = accuracy_score(true_labels, ensemble_labels)\n",
    "\n",
    "#print(\"Ensemble Loss (CNN + MobileNet + KNN):\", ensemble_loss_combined)\n",
    "print(\"Ensemble Accuracy (CNN + MobileNet + KNN):\", ensemble_accuracy_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predicted_labels = np.argmax(combined_predictions, axis=1)\n",
    "\n",
    "true_labels = test_david_generator.classes\n",
    "\n",
    "f1 = f1_score(true_labels, ensemble_predicted_labels, average='weighted')\n",
    "recall = recall_score(true_labels, ensemble_predicted_labels, average='weighted')\n",
    "accuracy = accuracy_score(true_labels, ensemble_predicted_labels)\n",
    "\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
